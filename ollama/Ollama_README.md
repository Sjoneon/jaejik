# Ollama ì‚¬ìš© ê°€ì´ë“œ

## ëª©ì°¨
- [Ollamaë€?](#ollamaë€)
- [ì„¤ì¹˜ ë°©ë²•](#ì„¤ì¹˜-ë°©ë²•)
  - [Windows PATH ì„¤ì • ë¬¸ì œ í•´ê²°](#ï¸-windowsì—ì„œ-ollama-ëª…ë ¹ì„-ì°¾ì„-ìˆ˜-ì—†ìŠµë‹ˆë‹¤-ì˜¤ë¥˜-í•´ê²°)
- [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
- [Python API ì‚¬ìš©](#python-api-ì‚¬ìš©)
- [ì£¼ìš” ëª…ë ¹ì–´](#ì£¼ìš”-ëª…ë ¹ì–´)
- [API ë ˆí¼ëŸ°ìŠ¤](#api-ë ˆí¼ëŸ°ìŠ¤)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## Ollamaë€?

**Ollama**ëŠ” ë¡œì»¬ì—ì„œ LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì„ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- ğŸš€ **ê°„í¸í•œ ì„¤ì¹˜**: í•œ ì¤„ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
- ğŸ’» **ë¡œì»¬ ì‹¤í–‰**: ì¸í„°ë„· ì—°ê²° ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥
- ğŸ”’ **í”„ë¼ì´ë²„ì‹œ**: ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ
- ğŸ¯ **ë‹¤ì–‘í•œ ëª¨ë¸**: Llama, Gemma, Mistral, Qwen ë“± ì§€ì›
- ğŸŒ **REST API**: HTTP APIë¡œ ë‹¤ì–‘í•œ ì–¸ì–´ì—ì„œ í™œìš© ê°€ëŠ¥

### ì§€ì› ëª¨ë¸ (2025ë…„ ê¸°ì¤€)
- **Gemma 3** (1B, 2B, 7B)
- **Llama 3.3** (1B, 3B, 70B)
- **Qwen 2.5** (0.5B~72B)
- **Mistral** (7B)
- **DeepSeek** (1.3B, 7B)
- ê¸°íƒ€ 100+ ëª¨ë¸

---

## ì„¤ì¹˜ ë°©ë²•

### Windows
```bash
# 1. Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
# https://ollama.com/download ì—ì„œ Windows ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ

# 2. ì„¤ì¹˜ í›„ í„°ë¯¸ë„ì—ì„œ í™•ì¸
ollama --version
```

#### âš ï¸ Windowsì—ì„œ "ollama ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì˜¤ë¥˜ í•´ê²°

ì„¤ì¹˜ í›„ í„°ë¯¸ë„ì—ì„œ `ollama` ëª…ë ¹ì–´ê°€ ì¸ì‹ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” PATH í™˜ê²½ ë³€ìˆ˜ì— Ollamaê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

**1ë‹¨ê³„: Ollama ì„¤ì¹˜ ìœ„ì¹˜ í™•ì¸**

ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ ìœ„ì¹˜ì— ì„¤ì¹˜ë©ë‹ˆë‹¤:
```
C:\Users\ì‚¬ìš©ìëª…\AppData\Local\Programs\Ollama
```

í™•ì¸ ë°©ë²•:
```cmd
# CMDì—ì„œ ì‹¤í–‰
dir "%LOCALAPPDATA%\Programs\Ollama"
```

**2ë‹¨ê³„: PATH í™˜ê²½ ë³€ìˆ˜ì— ì¶”ê°€ (ì˜êµ¬ì  í•´ê²°)**

ë°©ë²• A - GUIë¡œ ì„¤ì • (ê¶Œì¥):
1. `Win + R` í‚¤ë¥¼ ëˆ„ë¥´ê³  `sysdm.cpl` ì…ë ¥ í›„ ì—”í„°
2. **ê³ ê¸‰** íƒ­ â†’ **í™˜ê²½ ë³€ìˆ˜** í´ë¦­
3. **ì‚¬ìš©ì ë³€ìˆ˜** ì„¹ì…˜ì—ì„œ `Path` ì„ íƒ â†’ **í¸ì§‘** í´ë¦­
4. **ìƒˆë¡œ ë§Œë“¤ê¸°** í´ë¦­
5. `C:\Users\ì‚¬ìš©ìëª…\AppData\Local\Programs\Ollama` ì…ë ¥
   - `ì‚¬ìš©ìëª…`ì„ ë³¸ì¸ì˜ ì‚¬ìš©ìëª…ìœ¼ë¡œ ë³€ê²½
6. **í™•ì¸** â†’ **í™•ì¸** â†’ **í™•ì¸**
7. **í„°ë¯¸ë„ì„ ì™„ì „íˆ ë‹«ê³  ë‹¤ì‹œ ì—´ê¸°**

ë°©ë²• B - PowerShellë¡œ ì„¤ì •:
```powershell
# PowerShellì„ ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰
$ollamaPath = "$env:LOCALAPPDATA\Programs\Ollama"
[Environment]::SetEnvironmentVariable("Path", $env:Path + ";$ollamaPath", "User")

# í„°ë¯¸ë„ ì¬ì‹œì‘ í›„ í™•ì¸
ollama --version
```

**3ë‹¨ê³„: ì„ì‹œ ì‚¬ìš© (í˜„ì¬ ì„¸ì…˜ë§Œ)**

í„°ë¯¸ë„ì„ ì¬ì‹œì‘í•˜ê³  ì‹¶ì§€ ì•Šë‹¤ë©´:
```cmd
# CMDì—ì„œ
set PATH=%PATH%;C:\Users\ì‚¬ìš©ìëª…\AppData\Local\Programs\Ollama
ollama --version

# PowerShellì—ì„œ
$env:Path += ";$env:LOCALAPPDATA\Programs\Ollama"
ollama --version
```

**4ë‹¨ê³„: ì „ì²´ ê²½ë¡œë¡œ ì‹¤í–‰ (PATH ì„¤ì • ì—†ì´)**

```cmd
# ì „ì²´ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì—¬ ì§ì ‘ ì‹¤í–‰
C:\Users\ì‚¬ìš©ìëª…\AppData\Local\Programs\Ollama\ollama.exe --version
C:\Users\ì‚¬ìš©ìëª…\AppData\Local\Programs\Ollama\ollama.exe list
```

**í™•ì¸ ë°©ë²•:**
```cmd
# ì–´ëŠ í„°ë¯¸ë„ì—ì„œë“  ì‹¤í–‰
ollama --version

# ì¶œë ¥ ì˜ˆì‹œ:
# ollama version is 0.12.11
```

### macOS
```bash
# Homebrewë¥¼ í†µí•œ ì„¤ì¹˜
brew install ollama

# ë˜ëŠ” ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ
# https://ollama.com/download
```

### Linux
```bash
# í•œ ì¤„ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
curl -fsSL https://ollama.com/install.sh | sh

# ìˆ˜ë™ ì„¤ì¹˜
wget https://ollama.com/download/linux
sudo install linux /usr/local/bin/ollama
```

---

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
```bash
# Gemma 3 1B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma3:1b

# ëª¨ë¸ ì‹¤í–‰ (ëŒ€í™”í˜• ëª¨ë“œ)
ollama run gemma3:1b

# ì¢…ë£Œ: /bye ì…ë ¥
```

### 2. ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
```bash
# ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
ollama list

# ì¶œë ¥ ì˜ˆì‹œ:
# NAME              ID            SIZE      MODIFIED
# gemma3:1b         a1b2c3d4      815 MB    2 days ago
```

### 3. ëª¨ë¸ ì‚­ì œ
```bash
# íŠ¹ì • ëª¨ë¸ ì‚­ì œ
ollama rm gemma3:1b
```

### 4. ì„œë²„ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
```bash
# Ollama ì„œë²„ ì‹œì‘ (ê¸°ë³¸ í¬íŠ¸: 11434)
ollama serve

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (Linux/macOS)
ollama serve &

# Windowsì—ì„œëŠ” ìë™ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì„œë¹„ìŠ¤ë¡œ ì‹¤í–‰ë¨
```

---

## Python API ì‚¬ìš©

### ì„¤ì¹˜
```bash
pip install requests
```

### ê¸°ë³¸ ì˜ˆì œ

#### 1. ì„œë²„ ìƒíƒœ í™•ì¸
```python
import requests

OLLAMA_URL = "http://localhost:11434"

# ì„œë²„ ìƒíƒœ í™•ì¸
response = requests.get(f"{OLLAMA_URL}/")
print("ì„œë²„ ìƒíƒœ:", response.status_code)  # 200 = ì •ìƒ
```

#### 2. í…ìŠ¤íŠ¸ ìƒì„± (Generate API)
```python
import requests
import json

def generate_text(prompt, model="gemma3:1b"):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": True  # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹
        },
        stream=True
    )
    
    print("ì‘ë‹µ: ", end="")
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if 'response' in data:
                print(data['response'], end='', flush=True)
            if data.get('done', False):
                print("\n")
                # ì„±ëŠ¥ í†µê³„
                print(f"ì†Œìš” ì‹œê°„: {data.get('total_duration', 0) / 1e9:.2f}ì´ˆ")
                break

# ì‚¬ìš© ì˜ˆì‹œ
generate_text("íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ë§Œ ì„¤ëª…í•´ì¤˜")
```

#### 3. ëŒ€í™” (Chat API)
```python
def chat(messages, model="gemma3:1b"):
    """
    messages: [{"role": "user", "content": "ì§ˆë¬¸"}]
    """
    response = requests.post(
        "http://localhost:11434/api/chat",
        json={
            "model": model,
            "messages": messages,
            "stream": True
        },
        stream=True
    )
    
    full_response = ""
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if 'message' in data:
                chunk = data['message'].get('content', '')
                print(chunk, end='', flush=True)
                full_response += chunk
            if data.get('done', False):
                print("\n")
                break
    
    return full_response

# ëŒ€í™” ì˜ˆì‹œ
conversation = [
    {"role": "user", "content": "ì•ˆë…•! ë‚˜ëŠ” ì² ìˆ˜ì•¼."},
]
response1 = chat(conversation)

# ëŒ€í™” ì´ì–´ì„œ ì§„í–‰
conversation.append({"role": "assistant", "content": response1})
conversation.append({"role": "user", "content": "ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?"})
response2 = chat(conversation)
```

#### 4. ë¹„ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹
```python
def generate_non_stream(prompt, model="gemma3:1b"):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False  # í•œ ë²ˆì— ì „ì²´ ì‘ë‹µ ë°›ê¸°
        }
    )
    
    if response.status_code == 200:
        data = response.json()
        return data['response']
    else:
        return None

# ì‚¬ìš© ì˜ˆì‹œ
result = generate_non_stream("ê°„ë‹¨íˆ ìê¸°ì†Œê°œ í•´ì¤˜")
print(result)
```

### ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```bash
# í”„ë¡œì íŠ¸ì— í¬í•¨ëœ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python ollama/test_ollama_gemma3.py
```

---

## ì£¼ìš” ëª…ë ¹ì–´

### ëª¨ë¸ ê´€ë¦¬
```bash
# ëª¨ë¸ ê²€ìƒ‰
ollama search gemma

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma3:1b
ollama pull llama3.3:1b

# ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
ollama list

# ëª¨ë¸ ì •ë³´ í™•ì¸
ollama show gemma3:1b

# ëª¨ë¸ ì‚­ì œ
ollama rm gemma3:1b
```

### ì‹¤í–‰ ë° ëŒ€í™”
```bash
# ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰
ollama run gemma3:1b

# í•œ ë²ˆë§Œ ì§ˆë¬¸í•˜ê³  ì¢…ë£Œ
ollama run gemma3:1b "íŒŒì´ì¬ì´ë€?"

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ì‹¤í–‰
ollama run gemma3:1b --system "You are a helpful coding assistant"
```

### ì„œë²„ ê´€ë¦¬
```bash
# ì„œë²„ ì‹œì‘
ollama serve

# í¬íŠ¸ ë³€ê²½í•˜ì—¬ ì‹œì‘ (í™˜ê²½ë³€ìˆ˜)
OLLAMA_HOST=0.0.0.0:8080 ollama serve

# Windowsì—ì„œ í¬íŠ¸ ë³€ê²½
set OLLAMA_HOST=0.0.0.0:8080
ollama serve
```

---

## API ë ˆí¼ëŸ°ìŠ¤

### ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡

| ì—”ë“œí¬ì¸íŠ¸ | ë©”ì„œë“œ | ì„¤ëª… |
|-----------|--------|------|
| `/` | GET | ì„œë²„ ìƒíƒœ í™•ì¸ |
| `/api/generate` | POST | í…ìŠ¤íŠ¸ ìƒì„± |
| `/api/chat` | POST | ëŒ€í™”í˜• ì‘ë‹µ |
| `/api/tags` | GET | ëª¨ë¸ ëª©ë¡ ì¡°íšŒ |
| `/api/pull` | POST | ëª¨ë¸ ë‹¤ìš´ë¡œë“œ |
| `/api/show` | POST | ëª¨ë¸ ì •ë³´ ì¡°íšŒ |
| `/api/delete` | DELETE | ëª¨ë¸ ì‚­ì œ |

### Generate API íŒŒë¼ë¯¸í„°

```python
{
    "model": "gemma3:1b",           # í•„ìˆ˜: ëª¨ë¸ ì´ë¦„
    "prompt": "ì§ˆë¬¸ ë‚´ìš©",           # í•„ìˆ˜: í”„ë¡¬í”„íŠ¸
    "stream": true,                  # ì˜µì…˜: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€ (ê¸°ë³¸: true)
    "options": {
        "temperature": 0.7,          # ìƒì„± ë‹¤ì–‘ì„± (0.0~2.0, ê¸°ë³¸: 0.8)
        "top_k": 40,                 # Top-K ìƒ˜í”Œë§ (ê¸°ë³¸: 40)
        "top_p": 0.9,                # Top-P ìƒ˜í”Œë§ (ê¸°ë³¸: 0.9)
        "num_predict": 128,          # ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 128)
        "stop": ["\n", "END"]        # ì¤‘ë‹¨ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    },
    "system": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",     # ì˜µì…˜: ì‹œìŠ¤í…œ ë©”ì‹œì§€
    "context": []                    # ì˜µì…˜: ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
}
```

### Chat API íŒŒë¼ë¯¸í„°

```python
{
    "model": "gemma3:1b",
    "messages": [
        {"role": "system", "content": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"},
        {"role": "user", "content": "ì§ˆë¬¸"},
        {"role": "assistant", "content": "ì´ì „ ì‘ë‹µ"},
        {"role": "user", "content": "ë‹¤ìŒ ì§ˆë¬¸"}
    ],
    "stream": true,
    "options": {
        "temperature": 0.7,
        "top_p": 0.9
    }
}
```

### ì‘ë‹µ í˜•ì‹

#### Stream ì‘ë‹µ (stream=true)
```json
{"model":"gemma3:1b","created_at":"2025-11-16T...","response":"ì•ˆ","done":false}
{"model":"gemma3:1b","created_at":"2025-11-16T...","response":"ë…•","done":false}
{"model":"gemma3:1b","created_at":"2025-11-16T...","response":"í•˜","done":false}
{
    "model": "gemma3:1b",
    "created_at": "2025-11-16T...",
    "response": "",
    "done": true,
    "total_duration": 2270000000,
    "load_duration": 50000000,
    "prompt_eval_count": 10,
    "eval_count": 197,
    "eval_duration": 2200000000
}
```

#### Non-stream ì‘ë‹µ (stream=false)
```json
{
    "model": "gemma3:1b",
    "created_at": "2025-11-16T...",
    "response": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤...",
    "done": true,
    "total_duration": 2270000000,
    "eval_count": 197
}
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ëª¨ë¸ ì„ íƒ
```bash
# ìš©ë„ë³„ ê¶Œì¥ ëª¨ë¸
# - ë¹ ë¥¸ ì‘ë‹µ í•„ìš”: gemma3:1b, qwen2.5:0.5b
# - ê· í˜•ì¡íŒ ì„±ëŠ¥: gemma3:2b, llama3.3:3b
# - ë†’ì€ í’ˆì§ˆ: gemma3:7b, llama3.3:8b
```

### 2. íŒŒë¼ë¯¸í„° íŠœë‹
```python
# ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
options = {
    "num_predict": 64,        # ì§§ì€ ì‘ë‹µ
    "temperature": 0.5,       # ë‚®ì€ ë‹¤ì–‘ì„±
    "top_k": 20,             # ì‘ì€ ìƒ˜í”Œë§
}

# ì°½ì˜ì ì¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
options = {
    "num_predict": 256,       # ê¸´ ì‘ë‹µ
    "temperature": 1.0,       # ë†’ì€ ë‹¤ì–‘ì„±
    "top_p": 0.95,           # ë„“ì€ ìƒ˜í”Œë§
}
```

### 3. GPU ì‚¬ìš© (NVIDIA)
```bash
# OllamaëŠ” ìë™ìœ¼ë¡œ GPUë¥¼ ê°ì§€í•˜ê³  ì‚¬ìš©
# CUDA 11.8+ í•„ìš”

# GPU ì‚¬ìš© í™•ì¸
nvidia-smi

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í•„ìš”ì‹œ)
# Linux/macOS:
export CUDA_VISIBLE_DEVICES=0

# Windows:
set CUDA_VISIBLE_DEVICES=0
```

### 4. ë©”ëª¨ë¦¬ ê´€ë¦¬
```bash
# ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ì„¤ì •
# Linux/macOS:
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_MAX_QUEUE=512

# Windows:
set OLLAMA_MAX_LOADED_MODELS=2
set OLLAMA_MAX_QUEUE=512
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 0: Windowsì—ì„œ "ollama ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì˜¤ë¥˜
```cmd
# ì›ì¸: PATH í™˜ê²½ ë³€ìˆ˜ì— Ollamaê°€ ë“±ë¡ë˜ì§€ ì•ŠìŒ
# í•´ê²°: ìœ„ [ì„¤ì¹˜ ë°©ë²• - Windows PATH ì„¤ì • ë¬¸ì œ í•´ê²°] ì„¹ì…˜ ì°¸ì¡°

# ë¹ ë¥¸ ì„ì‹œ í•´ê²° (í˜„ì¬ ì„¸ì…˜ë§Œ):
set PATH=%PATH%;%LOCALAPPDATA%\Programs\Ollama
ollama --version

# ì˜êµ¬ì  í•´ê²°:
# Win + R â†’ sysdm.cpl â†’ ê³ ê¸‰ â†’ í™˜ê²½ ë³€ìˆ˜ â†’ Path í¸ì§‘
# ì¶”ê°€: C:\Users\ì‚¬ìš©ìëª…\AppData\Local\Programs\Ollama
```

### ë¬¸ì œ 1: "Connection refused" ì˜¤ë¥˜
```bash
# ì›ì¸: Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
# í•´ê²°:
ollama serve

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ í™•ì¸
# Windows: ì‘ì—… ê´€ë¦¬ìì—ì„œ ollama.exe í™•ì¸
# Linux/macOS: 
ps aux | grep ollama
```

### ë¬¸ì œ 2: í¬íŠ¸ ì¶©ëŒ (11434 í¬íŠ¸ ì‚¬ìš© ì¤‘)
```bash
# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹¤í–‰
# Linux/macOS:
OLLAMA_HOST=0.0.0.0:8080 ollama serve

# Windows:
set OLLAMA_HOST=0.0.0.0:8080
ollama serve

# Python ì½”ë“œë„ ìˆ˜ì •
OLLAMA_URL = "http://localhost:8080"
```

### ë¬¸ì œ 3: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# ë„¤íŠ¸ì›Œí¬ í™•ì¸
ping ollama.com

# ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
# 1. ëª¨ë¸ íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
# 2. ~/.ollama/models/ ë””ë ‰í† ë¦¬ì— ë°°ì¹˜
# Windows: C:\Users\ì‚¬ìš©ìëª…\.ollama\models\
```

### ë¬¸ì œ 4: GPU ì¸ì‹ ì•ˆ ë¨
```bash
# NVIDIA GPU ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA ë²„ì „ í™•ì¸ (11.8 ì´ìƒ í•„ìš”)
nvcc --version

# Ollama ì¬ì„¤ì¹˜ (GPU ë²„ì „)
# https://ollama.com/download
```

### ë¬¸ì œ 5: ì‘ë‹µì´ ë„ˆë¬´ ëŠë¦¼
```bash
# 1. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull gemma3:1b  # ëŒ€ì‹  2bë‚˜ 7b ëŒ€ì‹ 

# 2. num_predict ì¤„ì´ê¸°
options = {"num_predict": 64}

# 3. GPU ì‚¬ìš© í™•ì¸
# CPU ì‚¬ìš© ì‹œ GPUë¡œ ì „í™˜
```

### ë¬¸ì œ 6: Pythonì—ì„œ í•œê¸€ ê¹¨ì§
```python
# UTF-8 ì¸ì½”ë”© ëª…ì‹œ
response = requests.post(
    url,
    json=data,
    headers={"Content-Type": "application/json; charset=utf-8"}
)

# ë˜ëŠ” ì‘ë‹µ ì¸ì½”ë”© ì„¤ì •
response.encoding = 'utf-8'
```

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ê³µì‹ ë¬¸ì„œ
- **ê³µì‹ ì‚¬ì´íŠ¸**: https://ollama.com
- **GitHub**: https://github.com/ollama/ollama
- **API ë¬¸ì„œ**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬**: https://ollama.com/library

### ì»¤ë®¤ë‹ˆí‹°
- **Discord**: https://discord.gg/ollama
- **Reddit**: r/ollama
- **GitHub Discussions**: https://github.com/ollama/ollama/discussions

### ê´€ë ¨ í”„ë¡œì íŠ¸
- **Open WebUI**: Ollamaìš© ì›¹ ì¸í„°í˜ì´ìŠ¤
- **LangChain**: Ollama í†µí•© ì§€ì›
- **LlamaIndex**: Ollamaë¥¼ í†µí•œ RAG êµ¬í˜„

---

## ë¼ì´ì„ ìŠ¤ ë° ì£¼ì˜ì‚¬í•­

### Ollama
- **ë¼ì´ì„ ìŠ¤**: MIT License
- **ìƒì—…ì  ì‚¬ìš©**: ê°€ëŠ¥

### ëª¨ë¸ ë¼ì´ì„ ìŠ¤
ê° ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ ë¼ì´ì„ ìŠ¤ê°€ ì ìš©ë©ë‹ˆë‹¤:
- **Gemma**: Googleì˜ Gemma Terms of Use
- **Llama**: Metaì˜ Llama 3 Community License
- **Qwen**: Apache 2.0 (ì¼ë¶€ ëª¨ë¸)

ì‚¬ìš© ì „ ê° ëª¨ë¸ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.

---

## ë²„ì „ ì •ë³´

- **ë¬¸ì„œ ì‘ì„±ì¼**: 2025-11-16
- **Ollama ë²„ì „**: 0.5.0+
- **í…ŒìŠ¤íŠ¸ í™˜ê²½**: Windows 11, Python 3.11

---

## ì˜ˆì œ ì½”ë“œ ìœ„ì¹˜

```
í”„ë¡œì íŠ¸ êµ¬ì¡°:
koreaspray/
â”œâ”€â”€ ollama/
â”‚   â””â”€â”€ test_ollama_gemma3.py    # í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ Ollama_README.md              # ì´ ë¬¸ì„œ
â””â”€â”€ requirements.txt              # í•„ìš”í•œ íŒ¨í‚¤ì§€
```

### ë¹ ë¥¸ ì‹œì‘
```bash
# 1. Ollama ì„¤ì¹˜ (ìœ„ ì„¤ì¹˜ ë°©ë²• ì°¸ì¡°)
# Windows: https://ollama.com/download

# 2. PATH ì„¤ì • í™•ì¸ (Windowsë§Œ í•´ë‹¹)
ollama --version
# "ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì˜¤ë¥˜ ì‹œ â†’ ìœ„ Windows PATH ì„¤ì • ì„¹ì…˜ ì°¸ì¡°

# 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma3:1b

# 4. ì„œë²„ ì‹œì‘ (ìë™ ì‹¤í–‰ë˜ì§€ ì•Šì€ ê²½ìš°)
ollama serve

# 5. Python í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python ollama/test_ollama_gemma3.py
```

---

**ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ GitHub Issuesì— ì˜¬ë ¤ì£¼ì„¸ìš”!** ğŸš€

