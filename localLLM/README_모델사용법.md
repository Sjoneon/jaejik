# Gemma 3 1B IT ëª¨ë¸ - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
koreaspray/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ gemma-3-1b-it/          # ë¡œì»¬ ëª¨ë¸ íŒŒì¼ (1.90 GB)
â”‚       â”œâ”€â”€ model.safetensors   # ëª¨ë¸ ì›¨ì´íŠ¸
â”‚       â”œâ”€â”€ tokenizer.json      # í† í¬ë‚˜ì´ì €
â”‚       â””â”€â”€ ... (ì„¤ì • íŒŒì¼ë“¤)
â”‚
â”œâ”€â”€ test_gemma3_comprehensive.py  # ğŸ§ª ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ GEMMA3_ê°•ì˜ìë£Œ.md            # ğŸ“– ìƒì„¸ ê°•ì˜ ìë£Œ
â””â”€â”€ README_ëª¨ë¸ì‚¬ìš©ë²•.md          # ì´ íŒŒì¼

```

## ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

```bash
# ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ê¶Œì¥)
python test_gemma3_comprehensive.py

# ë‹¤ë¥¸ ëª¨ë¸ ê²½ë¡œë¡œ í…ŒìŠ¤íŠ¸
python test_gemma3_comprehensive.py --model-path /path/to/model
```

í…ŒìŠ¤íŠ¸ëŠ” ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:
- âœ… ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
- âœ… ëª¨ë¸ ë¡œë”© ë° ì‹œê°„ ì¸¡ì •
- âœ… 6ê°€ì§€ ë‹¤ì–‘í•œ ìƒì„± í…ŒìŠ¤íŠ¸ (í•œêµ­ì–´/ì˜ì–´, ìˆ˜í•™, ì½”ë“œ ë“±)
- âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
- âœ… ì„±ëŠ¥ ì¸¡ì • (5íšŒ ì—°ì† ìƒì„±)
- âœ… JSON ë¦¬í¬íŠ¸ ìë™ ìƒì„±

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ë¡œì»¬ ëª¨ë¸ë¡œ ì‹¤í–‰ (ê°•ì˜ìš© ê¶Œì¥)

```python
from transformers import pipeline
import torch

# ë¡œì»¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë“œ
pipe = pipeline(
    "text-generation",
    model="models/gemma-3-1b-it",
    device="cpu",
    dtype=torch.float32
)

# ì§ˆë¬¸í•˜ê¸°
messages = [[{
    "role": "user",
    "content": [{"type": "text", "text": "ì•ˆë…•í•˜ì„¸ìš”!"}]
}]]

output = pipe(messages, max_new_tokens=50)
print(output)
```

### 2. ì‘ë‹µ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜

```python
def extract_response(output):
    """ìƒì„±ëœ ì‘ë‹µì„ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        # ì¶œë ¥ êµ¬ì¡°: [[{'generated_text': [user_msg, assistant_msg]}]]
        messages = output[0][0]['generated_text']
        
        # assistant ë©”ì‹œì§€ ì°¾ê¸°
        for msg in reversed(messages):
            if msg.get('role') == 'assistant':
                return msg.get('content', '')
        
        return None
    except:
        return None

# ì‚¬ìš© ì˜ˆ
output = pipe(messages, max_new_tokens=50)
response = extract_response(output)
print(response)
```

## âœ… ë™ì‘ í™•ì¸ ì™„ë£Œ

- âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ì™„ë£Œ (1.90 GB)
- âœ… ë¡œì»¬ ê²½ë¡œ ë³µì‚¬: `models/gemma-3-1b-it/`
- âœ… CPU ë™ì‘ í…ŒìŠ¤íŠ¸: ì„±ê³µ
- âœ… ëª¨ë¸ ë¡œë”© ì‹œê°„: ì•½ 2.4ì´ˆ
- âœ… í…ìŠ¤íŠ¸ ìƒì„± ì†ë„: ì•½ 2.3ì´ˆ (30-50 í† í°)
- âœ… ì‘ë‹µ ì¶”ì¶œ: ì •ìƒ ì‘ë™

### í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì˜ˆì‹œ

```
ê°„ë‹¨í•œ ìˆ˜í•™ ë¬¸ì œ
  ì…ë ¥: What is 2 + 2?
  ì‘ë‹µ: 2 + 2 = 4
  ì‹œê°„: 2.25ì´ˆ
```

## ğŸ“¦ ê°•ì˜ ìë£Œ ë°°í¬

ì´ í´ë” ì „ì²´ë¥¼ USBë‚˜ í´ë¼ìš°ë“œë¡œ ê³µìœ í•˜ë©´ ë©ë‹ˆë‹¤:

```
koreaspray/
â”œâ”€â”€ models/gemma-3-1b-it/    # â† ì´ í´ë”ê°€ í•µì‹¬!
â””â”€â”€ GEMMA3_ê°•ì˜ìë£Œ.md        # â† ê°•ì˜ ìë£Œ
```

í•™ìƒë“¤ì€:
1. ì´ í´ë”ë¥¼ ë°›ì•„ì„œ
2. `pip install transformers torch accelerate`
3. `python test_local_model.py` ì‹¤í–‰

ì¸í„°ë„· ì—†ì´ë„ ë°”ë¡œ ë™ì‘í•©ë‹ˆë‹¤! ğŸ‰

## ğŸ“– ìƒì„¸ ë¬¸ì„œ

ë” ìì„¸í•œ ë‚´ìš©ì€ `GEMMA3_ê°•ì˜ìë£Œ.md`ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

- ë‹¤ì–‘í•œ ì‚¬ìš© ì˜ˆì œ
- ìµœì í™” íŒ
- ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
- API ì°¸ì¡°

