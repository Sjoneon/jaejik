"""
Gemma 3 1B IT ëª¨ë¸ ì¢…í•© í…ŒìŠ¤íŠ¸
ë¡œì»¬ ëª¨ë¸ì˜ CPU ë™ì‘ì„ ì²´ê³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')


class Gemma3Tester:
    """Gemma 3 ëª¨ë¸ í…ŒìŠ¤í„°"""
    
    def __init__(self, model_path: str = "models/gemma-3-1b-it"):
        self.model_path = Path(model_path).absolute()
        self.pipe = None
        self.test_results = []
        
    def print_header(self, text: str, char: str = "="):
        """í—¤ë” ì¶œë ¥"""
        print("\n" + char * 80)
        print(text.center(80))
        print(char * 80)
    
    def print_section(self, text: str):
        """ì„¹ì…˜ ì¶œë ¥"""
        print(f"\n{'â”€' * 80}")
        print(f"â–¶ {text}")
        print('â”€' * 80)
    
    def check_model_exists(self) -> bool:
        """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        self.print_section("1. ëª¨ë¸ íŒŒì¼ í™•ì¸")
        
        print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        
        if not self.model_path.exists():
            print(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = [
            "model.safetensors",
            "config.json",
            "tokenizer.json",
            "tokenizer.model"
        ]
        
        print("\ní•„ìˆ˜ íŒŒì¼ ì²´í¬:")
        all_exist = True
        total_size = 0
        
        for filename in required_files:
            filepath = self.model_path / filename
            if filepath.exists():
                size = filepath.stat().st_size
                total_size += size
                size_mb = size / (1024 * 1024)
                size_gb = size / (1024 * 1024 * 1024)
                
                if size_gb >= 1:
                    size_str = f"{size_gb:.2f} GB"
                else:
                    size_str = f"{size_mb:.1f} MB"
                
                print(f"  âœ… {filename:30s} {size_str:>12s}")
            else:
                print(f"  âŒ {filename:30s} {'ì—†ìŒ':>12s}")
                all_exist = False
        
        total_gb = total_size / (1024 * 1024 * 1024)
        print(f"\nì´ í¬ê¸°: {total_gb:.2f} GB")
        
        if all_exist:
            print("\nâœ… ëª¨ë“  í•„ìˆ˜ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
        else:
            print("\nâŒ ì¼ë¶€ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return all_exist
    
    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë”©"""
        self.print_section("2. ëª¨ë¸ ë¡œë”©")
        
        try:
            print("ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì¤‘...")
            from transformers import pipeline
            import torch
            
            print(f"  âœ… PyTorch ë²„ì „: {torch.__version__}")
            print(f"  âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {'ì˜ˆ' if torch.cuda.is_available() else 'ì•„ë‹ˆì˜¤ (CPU ì „ìš©)'}")
            
            print(f"\nëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            print(f"  ê²½ë¡œ: {self.model_path}")
            
            start_time = time.time()
            
            self.pipe = pipeline(
                "text-generation",
                model=str(self.model_path),
                device="cpu",
                dtype=torch.float32
            )
            
            load_time = time.time() - start_time
            
            print(f"\nâœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"  ì†Œìš” ì‹œê°„: {load_time:.2f}ì´ˆ")
            
            self.test_results.append({
                "test": "ëª¨ë¸ ë¡œë”©",
                "status": "ì„±ê³µ",
                "time": load_time
            })
            
            return True
            
        except Exception as e:
            print(f"\nâŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.test_results.append({
                "test": "ëª¨ë¸ ë¡œë”©",
                "status": "ì‹¤íŒ¨",
                "error": str(e)
            })
            return False
    
    def extract_response(self, output) -> Optional[str]:
        """ìƒì„±ëœ ì‘ë‹µ ì¶”ì¶œ
        
        ì¶œë ¥ êµ¬ì¡°: [[{'generated_text': [user_msg, assistant_msg]}]]
        """
        try:
            # output[0][0]['generated_text']ë¡œ ì ‘ê·¼
            if not isinstance(output, list) or len(output) == 0:
                return None
            
            # ì²« ë²ˆì§¸ ë ˆë²¨ ë¦¬ìŠ¤íŠ¸
            first_level = output[0]
            if not isinstance(first_level, list) or len(first_level) == 0:
                return None
            
            # ë‘ ë²ˆì§¸ ë ˆë²¨ ë”•ì…”ë„ˆë¦¬
            second_level = first_level[0]
            if not isinstance(second_level, dict) or 'generated_text' not in second_level:
                return None
            
            # generated_textëŠ” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            messages = second_level['generated_text']
            if not isinstance(messages, list) or len(messages) == 0:
                return None
            
            # assistant ë©”ì‹œì§€ ì°¾ê¸° (ì—­ìˆœìœ¼ë¡œ ê²€ìƒ‰)
            for msg in reversed(messages):
                if isinstance(msg, dict) and msg.get('role') == 'assistant':
                    content = msg.get('content', '')
                    
                    # contentê°€ ë¬¸ìì—´ì´ë©´ ì§ì ‘ ë°˜í™˜
                    if isinstance(content, str):
                        return content
                    
                    # contentê°€ ë¦¬ìŠ¤íŠ¸ë©´ ì²« ë²ˆì§¸ ì•„ì´í…œì—ì„œ text ì¶”ì¶œ
                    elif isinstance(content, list) and len(content) > 0:
                        first_item = content[0]
                        if isinstance(first_item, dict):
                            return first_item.get('text', str(first_item))
                        return str(first_item)
            
            return None
            
        except Exception as e:
            print(f"  âš ï¸  ì‘ë‹µ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def test_generation(self, test_name: str, prompt: str, max_tokens: int = 50) -> Dict:
        """í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        print(f"\n  ğŸ“ {test_name}")
        print(f"      ì…ë ¥: {prompt}")
        
        try:
            messages = [[{
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }]]
            
            start_time = time.time()
            output = self.pipe(messages, max_new_tokens=max_tokens)
            gen_time = time.time() - start_time
            
            response = self.extract_response(output)
            
            if response:
                # ì‘ë‹µì„ 50ìë¡œ ì œí•œí•˜ì—¬ ì¶œë ¥
                display_response = response[:100] + "..." if len(response) > 100 else response
                print(f"      ì‘ë‹µ: {display_response}")
                print(f"      ì‹œê°„: {gen_time:.2f}ì´ˆ")
                
                return {
                    "test": test_name,
                    "prompt": prompt,
                    "response": response,
                    "time": gen_time,
                    "status": "ì„±ê³µ",
                    "tokens": max_tokens
                }
            else:
                print(f"      âŒ ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨")
                return {
                    "test": test_name,
                    "prompt": prompt,
                    "status": "ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨",
                    "time": gen_time
                }
                
        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {e}")
            return {
                "test": test_name,
                "prompt": prompt,
                "status": "ì‹¤íŒ¨",
                "error": str(e)
            }
    
    def run_generation_tests(self):
        """ë‹¤ì–‘í•œ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.print_section("3. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
        
        test_cases = [
            ("ê°„ë‹¨í•œ ìˆ˜í•™ ë¬¸ì œ", "What is 2 + 2?", 20),
            ("í•œêµ­ì–´ ì¸ì‚¬", "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.", 30),
            ("ì˜ì–´ ì¸ì‚¬", "Hello! Please introduce yourself briefly.", 40),
            ("ì½”ë“œ ìš”ì²­", "Write a simple Python hello world.", 50),
            ("í•œêµ­ì–´ ì„¤ëª…", "Pythonì´ ë­”ê°€ìš”? í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.", 50),
            ("ê¸´ ì‘ë‹µ", "Explain what artificial intelligence is.", 100),
        ]
        
        print(f"\nì´ {len(test_cases)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ì¤‘...\n")
        
        for test_name, prompt, max_tokens in test_cases:
            result = self.test_generation(test_name, prompt, max_tokens)
            self.test_results.append(result)
            time.sleep(0.5)  # í…ŒìŠ¤íŠ¸ ê°„ ì§§ì€ ëŒ€ê¸°
    
    def test_system_prompt(self):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸"""
        self.print_section("4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸")
        
        print("\n  ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨ í…ŒìŠ¤íŠ¸")
        
        try:
            messages = [[
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are a helpful AI assistant that always responds in Korean."}]
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": "What is AI?"}]
                }
            ]]
            
            start_time = time.time()
            output = self.pipe(messages, max_new_tokens=60)
            gen_time = time.time() - start_time
            
            response = self.extract_response(output)
            
            if response:
                display_response = response[:100] + "..." if len(response) > 100 else response
                print(f"      ì‘ë‹µ: {display_response}")
                print(f"      ì‹œê°„: {gen_time:.2f}ì´ˆ")
                
                self.test_results.append({
                    "test": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                    "status": "ì„±ê³µ",
                    "time": gen_time
                })
                print("\n  âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ìƒ ì‘ë™")
            else:
                print("      âŒ ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨")
                self.test_results.append({
                    "test": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                    "status": "ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨"
                })
                
        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {e}")
            self.test_results.append({
                "test": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                "status": "ì‹¤íŒ¨",
                "error": str(e)
            })
    
    def test_performance(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        self.print_section("5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        
        print("\n  âš¡ ì—°ì† ìƒì„± ì„±ëŠ¥ ì¸¡ì • (5íšŒ)")
        
        times = []
        test_prompt = "Count from 1 to 5."
        
        for i in range(5):
            messages = [[{
                "role": "user",
                "content": [{"type": "text", "text": test_prompt}]
            }]]
            
            start_time = time.time()
            output = self.pipe(messages, max_new_tokens=30)
            gen_time = time.time() - start_time
            times.append(gen_time)
            
            print(f"      ì‹œë„ {i+1}: {gen_time:.2f}ì´ˆ")
        
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        print(f"\n  ğŸ“Š ì„±ëŠ¥ í†µê³„:")
        print(f"      í‰ê· : {avg_time:.2f}ì´ˆ")
        print(f"      ìµœì†Œ: {min_time:.2f}ì´ˆ")
        print(f"      ìµœëŒ€: {max_time:.2f}ì´ˆ")
        
        self.test_results.append({
            "test": "ì„±ëŠ¥ ì¸¡ì •",
            "status": "ì„±ê³µ",
            "avg_time": avg_time,
            "min_time": min_time,
            "max_time": max_time
        })
    
    def generate_report(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.print_header("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸", "=")
        
        # í†µê³„
        total_tests = len(self.test_results)
        successful = sum(1 for r in self.test_results if r.get("status") == "ì„±ê³µ")
        failed = total_tests - successful
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  â€¢ ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"  â€¢ ì„±ê³µ: {successful}ê°œ âœ…")
        print(f"  â€¢ ì‹¤íŒ¨: {failed}ê°œ {'âŒ' if failed > 0 else 'âœ…'}")
        
        # ì„±ëŠ¥ ìš”ì•½
        gen_results = [r for r in self.test_results if "time" in r and r.get("status") == "ì„±ê³µ" and "prompt" in r]
        
        if gen_results:
            times = [r["time"] for r in gen_results]
            avg_time = sum(times) / len(times)
            
            print(f"\nâš¡ ìƒì„± ì„±ëŠ¥:")
            print(f"  â€¢ í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.2f}ì´ˆ")
            print(f"  â€¢ ìµœì†Œ ìƒì„± ì‹œê°„: {min(times):.2f}ì´ˆ")
            print(f"  â€¢ ìµœëŒ€ ìƒì„± ì‹œê°„: {max(times):.2f}ì´ˆ")
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸
        failed_tests = [r for r in self.test_results if r.get("status") != "ì„±ê³µ"]
        if failed_tests:
            print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for test in failed_tests:
                print(f"  â€¢ {test.get('test', 'Unknown')}: {test.get('error', test.get('status'))}")
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_file = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "timestamp": datetime.now().isoformat(),
                    "model_path": str(self.model_path),
                    "summary": {
                        "total": total_tests,
                        "successful": successful,
                        "failed": failed
                    },
                    "results": self.test_results
                }, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        except Exception as e:
            print(f"\nâš ï¸  ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… ê²°ê³¼
        print("\n" + "=" * 80)
        if failed == 0:
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! CPUì—ì„œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.".center(80))
        else:
            print(f"âš ï¸  {failed}ê°œì˜ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.".center(80))
        print("=" * 80 + "\n")
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.print_header("Gemma 3 1B IT - ì¢…í•© í…ŒìŠ¤íŠ¸", "=")
        
        print(f"\ní…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. ëª¨ë¸ íŒŒì¼ í™•ì¸
        if not self.check_model_exists():
            print("\nâŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return False
        
        # 2. ëª¨ë¸ ë¡œë”©
        if not self.load_model():
            print("\nâŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return False
        
        # 3. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        self.run_generation_tests()
        
        # 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        self.test_system_prompt()
        
        # 5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self.test_performance()
        
        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report()
        
        return True


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Gemma 3 1B IT ëª¨ë¸ ì¢…í•© í…ŒìŠ¤íŠ¸")
    parser.add_argument(
        "--model-path",
        type=str,
        default="models/gemma-3-1b-it",
        help="ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: models/gemma-3-1b-it)"
    )
    
    args = parser.parse_args()
    
    tester = Gemma3Tester(model_path=args.model_path)
    
    try:
        tester.run_all_tests()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

