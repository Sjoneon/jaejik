# Gemma 3 1B IT ëª¨ë¸ - ê°•ì˜ ìë£Œ

## ğŸ“¦ ëª¨ë¸ ì •ë³´

- **ëª¨ë¸ëª…**: google/gemma-3-1b-it
- **í¬ê¸°**: 1.90 GB
- **íƒ€ì…**: Instruction-tuned í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸
- **ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°**: 32K í† í° (1B ëª¨ë¸)
- **ë¼ì´ì„ ìŠ¤**: Gemma License (ìŠ¹ì¸ í•„ìš”)

## ğŸ“ ë¡œì»¬ ëª¨ë¸ ìœ„ì¹˜

### í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œ
```
C:\Users\Pc\koreaspray\models\gemma-3-1b-it\
```

### í¬í•¨ íŒŒì¼
- `model.safetensors` (1.86 GB) - ëª¨ë¸ ì›¨ì´íŠ¸
- `tokenizer.json` (31.8 MB) - í† í¬ë‚˜ì´ì €
- `tokenizer.model` (4.5 MB)
- `config.json` - ëª¨ë¸ ì„¤ì •
- `generation_config.json` - ìƒì„± ì„¤ì •
- `tokenizer_config.json` - í† í¬ë‚˜ì´ì € ì„¤ì •
- `special_tokens_map.json` - íŠ¹ìˆ˜ í† í° ë§µí•‘
- `added_tokens.json` - ì¶”ê°€ í† í°

## âœ… CPU ë™ì‘ í…ŒìŠ¤íŠ¸ ê²°ê³¼

### ì„±ëŠ¥
- âœ… **ëª¨ë¸ ë¡œë”©**: ì•½ 2.8ì´ˆ
- âœ… **í…ìŠ¤íŠ¸ ìƒì„±**: ì•½ 1.7ì´ˆ (30 í† í°)
- âœ… **CPU ì „ìš©**: GPU ì—†ì´ë„ ì •ìƒ ì‘ë™

### í™˜ê²½
- PyTorch: 2.9.1+cpu
- Transformers: 4.57.1
- Device: CPU
- OS: Windows 11

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. ì˜¨ë¼ì¸ ëª¨ë¸ ì‚¬ìš© (Hugging Face)

```python
from transformers import pipeline
import torch

# Hugging Faceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
pipe = pipeline(
    "text-generation",
    model="google/gemma-3-1b-it",
    device="cpu",
    dtype=torch.float32
)

messages = [[{
    "role": "user",
    "content": [{"type": "text", "text": "ì•ˆë…•í•˜ì„¸ìš”!"}]
}]]

output = pipe(messages, max_new_tokens=50)
```

**ì¥ì **: í•­ìƒ ìµœì‹  ë²„ì „
**ë‹¨ì **: ì¸í„°ë„· í•„ìš”, Hugging Face ë¡œê·¸ì¸ í•„ìš”

### 2. ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (ê¶Œì¥ - ê°•ì˜ìš©)

```python
from transformers import pipeline
import torch

# ë¡œì»¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë“œ
model_path = "models/gemma-3-1b-it"

pipe = pipeline(
    "text-generation",
    model=model_path,
    device="cpu",
    dtype=torch.float32
)

messages = [[{
    "role": "user",
    "content": [{"type": "text", "text": "What is 2+2?"}]
}]]

output = pipe(messages, max_new_tokens=50)

# ì‘ë‹µ ì¶”ì¶œ
if isinstance(output, list) and len(output) > 0:
    result = output[0]
    if isinstance(result, list) and len(result) > 0:
        last_message = result[-1]
        if 'content' in last_message:
            content = last_message['content']
            if isinstance(content, list):
                print(content[0]['text'])
```

**ì¥ì **: 
- ì¸í„°ë„· ë¶ˆí•„ìš”
- ê°•ì˜ì‹¤ì—ì„œ ì•ˆì •ì 
- ë¹ ë¥¸ ë¡œë”© (ìºì‹œ í™œìš©)

**ë‹¨ì **: 
- ì´ˆê¸° 2GB ë‹¤ìš´ë¡œë“œ í•„ìš”
- ë””ìŠ¤í¬ ê³µê°„ ì‚¬ìš©

## ğŸ“š ê°•ì˜ ìë£Œ ë°°í¬ ë°©ë²•

### ë°©ë²• 1: ëª¨ë¸ í´ë” ì§ì ‘ ë°°í¬

1. `models/gemma-3-1b-it` í´ë” ì „ì²´ë¥¼ USBë‚˜ í´ë¼ìš°ë“œì— ë³µì‚¬
2. í•™ìƒë“¤ì€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `models/` ë””ë ‰í† ë¦¬ ìƒì„±
3. ë°›ì€ `gemma-3-1b-it` í´ë”ë¥¼ `models/` ì•ˆì— ë³µì‚¬
4. ìœ„ì˜ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì½”ë“œë¡œ ì‹¤í–‰

### ë°©ë²• 2: Hugging Face ìºì‹œ í™œìš©

ì›ë³¸ ìºì‹œ ìœ„ì¹˜:
```
C:\Users\Pc\.cache\huggingface\hub\models--google--gemma-3-1b-it
```

ì´ í´ë”ë¥¼ í•™ìƒ PCì˜ ê°™ì€ ê²½ë¡œì— ë³µì‚¬í•˜ë©´ ìë™ ì¸ì‹ë©ë‹ˆë‹¤.

### ë°©ë²• 3: í•™ìƒë“¤ì´ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)

1. Hugging Face ê³„ì • ìƒì„±: https://huggingface.co
2. ëª¨ë¸ í˜ì´ì§€ì—ì„œ ë¼ì´ì„ ìŠ¤ ë™ì˜: https://huggingface.co/google/gemma-3-1b-it
3. Hugging Face í† í° ìƒì„± (READ ê¶Œí•œ)
4. ë¡œê·¸ì¸:
```python
from huggingface_hub import login
login(token="your_token_here")
```
5. ì²« ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ

**ì¥ì **: 
- ì €ì‘ê¶Œ ì•ˆì „
- ê°ì ìµœì‹  ë²„ì „ ì‚¬ìš©
- ì¬ë°°í¬ ë¬¸ì œ ì—†ìŒ

## ğŸ”§ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
pip install transformers torch accelerate sentencepiece protobuf
```

ë˜ëŠ” `requirements.txt`:
```
transformers>=4.50.0
torch>=2.0.0
accelerate
sentencepiece
protobuf
```

## ğŸ“ ì˜ˆì œ ì½”ë“œ

### ê°„ë‹¨í•œ ì±„íŒ…
```python
from transformers import pipeline
import torch

pipe = pipeline(
    "text-generation",
    model="models/gemma-3-1b-it",  # ë¡œì»¬ ê²½ë¡œ
    device="cpu",
    dtype=torch.float32
)

def chat(question):
    messages = [[{
        "role": "user",
        "content": [{"type": "text", "text": question}]
    }]]
    
    output = pipe(messages, max_new_tokens=100)
    
    # ì‘ë‹µ ì¶”ì¶œ
    if isinstance(output, list) and len(output) > 0:
        result = output[0]
        if isinstance(result, list) and len(result) > 0:
            last = result[-1]
            if 'content' in last:
                return last['content'][0]['text']
    return "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"

# ì‚¬ìš© ì˜ˆ
print(chat("Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"))
print(chat("1+1ì€?"))
```

### ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
```python
messages = [[
    {
        "role": "system",
        "content": [{"type": "text", "text": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."}]
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": "ì•ˆë…•í•˜ì„¸ìš”!"}]
    }
]]

output = pipe(messages, max_new_tokens=50)
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë¼ì´ì„ ìŠ¤**: Gemma License ë™ì˜ í•„ìš”
2. **CPU ì „ìš©**: GPU ì—†ì´ë„ ë™ì‘í•˜ì§€ë§Œ ëŠë¦¼
3. **ë©”ëª¨ë¦¬**: ìµœì†Œ 4GB RAM ê¶Œì¥
4. **ì²« ì‹¤í–‰**: ëª¨ë¸ ë¡œë”©ì— 2-5ì´ˆ ì†Œìš”
5. **ìƒì„± ì†ë„**: CPUì—ì„œ ì´ˆë‹¹ 5-10 í† í° ì •ë„

## ğŸš€ ìµœì í™” íŒ

### ë” ë¹ ë¥¸ ìƒì„±
```python
output = pipe(
    messages,
    max_new_tokens=50,
    do_sample=False,  # ê·¸ë¦¬ë”” ë””ì½”ë”© (ë” ë¹ ë¦„)
    num_beams=1       # ë¹” ì„œì¹˜ ë¹„í™œì„±í™”
)
```

### ë©”ëª¨ë¦¬ ì ˆì•½
```python
import torch

pipe = pipeline(
    "text-generation",
    model="models/gemma-3-1b-it",
    device="cpu",
    dtype=torch.float16,  # ë°˜ì •ë°€ë„ (ë©”ëª¨ë¦¬ ì ˆì•½)
    torch_dtype=torch.float16
)
```

## ğŸ“ ë¬¸ì œ í•´ê²°

### "Access denied" ì˜¤ë¥˜
â†’ Hugging Face ë¡œê·¸ì¸ í•„ìš”: https://huggingface.co/google/gemma-3-1b-it

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
â†’ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ, ë¸Œë¼ìš°ì € íƒ­ ë‹«ê¸°

### ë„ˆë¬´ ëŠë¦¼
â†’ max_new_tokens ê°’ ì¤„ì´ê¸° (100 â†’ 50)
â†’ GPUê°€ ìˆë‹¤ë©´ device="cuda" ì‚¬ìš©

### í•œê¸€ ê¹¨ì§
â†’ tokenizerëŠ” í•œêµ­ì–´ ì§€ì›í•¨ (140ê°œ ì–¸ì–´)
â†’ ì¶œë ¥ ì¸ì½”ë”© í™•ì¸: sys.stdout.reconfigure(encoding='utf-8')

## ğŸ“– ì°¸ê³  ìë£Œ

- Hugging Face ëª¨ë¸ í˜ì´ì§€: https://huggingface.co/google/gemma-3-1b-it
- Gemma ê³µì‹ ë¬¸ì„œ: https://ai.google.dev/gemma
- Transformers ë¬¸ì„œ: https://huggingface.co/docs/transformers

---

**ì‘ì„±ì¼**: 2025-11-16
**í…ŒìŠ¤íŠ¸ í™˜ê²½**: Windows 11, Python 3.11, CPU

